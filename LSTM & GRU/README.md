# 쓰기

#### 텍스트 데이터와 이미지 데이터 사이의 차이점

- 텍스트 데이터는 개별적인 데이터 조각으로 구성 반면 이미지 픽셀은 연속적인 색상 스펙트럼 위의 한 점 -> 텍스트 데이터는 일반적인 방식으로 역전파 적용 불가
- 텍스트 데이터는 시간 차원이 있지만 공간 차원은 없음 반면 이미지 데이터는 두 개의 공간 차원이 있고 시간 차원이 없음
- 텍스트 데이터는 개별 단위의 작은 변화에도 매우 민감
- 텍스트 데이터는 규칙 기반을 가진 문법 구조를 가짐

순차 데이터를 생성하는 데 가장 유용하고 성공적인 모델 중에 하나인 **순환 신경망과 LSTM층** 알아보기

## LSTM 네트워크 소개

- LSTM 네트워크는 순환 신경망(RNN) 의 한 종류
- RNN은 특정 타임스텝의 출력이 다음 타임스템 입력의 일부분으로 사용

## 첫 번째 LSTM 네트워크

데이터를 적절한 형태로 가공하기

### 토큰화

**토큰화** : 텍스트를 단어나 문자와 같은 개별 단위로 나누는 작업

#### 단어 토큰의 경우

- 모든 텍스트를 소문자로 변환
- 어휘사전(훈련 세트에 있는 고유한 단어의 모음)이 매우 클 수 있음 -> 희소한 단어는 별도의 토큰 보다 알려지지 않은 단어에 해당하는 토큰으로 바꾸기
- 단어에서 어간 추출하기(browse, browsing, browses는 모두 brows)
- 구두점(마침표와 쉼표)을 토큰화하거나 모두 제거해야 함
- 단어 토큰화를 사용하면 훈련 어휘 사전에 없는 단어는 모델이 예측 불가

#### 문자 토큰의 경우

- 모델이 문자의 시퀀스를 생성해 훈련 어휘 사전에 없는 새로운 단어 만들 수 있음 
- 대문자는 소문자로 바꾸거나 별도의 토큰으로 남길 수 있음
- 문자 토큰화를 사용하면 어휘 사전이 비교적 매우 작음 -> 마지막 출력층에 학습할 가중치 수가 적기에 훈련 속도에 유리

### 데이터셋 구축

LSTM 네트워크는 단어의 시퀀스가 주어지면 이 시퀀스의 다음 단어를 예측하도록 훈련

모델 훈련에 사용할 시퀀스 길이는 하이퍼파라미터

### 임베딩층

- 임베딩층은 기본적으로 각 토큰을 embedding_size 길이의 벡터로 변환하는 룩업 테이블 

  -> 이 층에서 학습되는 가중치의 개수는 어휘 사전의 크기에 embedding_size를 곱한 값 

- 입력 토큰을 원-핫 인코딩할 수 있지만 임베딩 층 더 선호

  -> 임베딩 층은 스스로 학습할 수 있기 때문에 성능 향상을 위해 모델이 토큰의 임베딩 방법을 자유롭게 결정할 수 있기 때문

### LSTM 층

- 시퀀스가 끝나면 이 층은 셀의 최종 은닉 상태를 출력하고 네트워크의 다음 층으로 전달

- 마지막 은닉 상태가 전반적인 이 층의 출력

## 새로운 텍스트 생성

**긴 텍스트 문자열 생성하는 방법**

1. 기존 단어의 시퀀스를 네트워크에 주입하고 다음 단어를 예측
2. 이 단어를 기존 시퀀스에 추가하고 그 과정을 반복

위의 네트워크는 샘플링할 수 있는 각 단어의 확률 출력 -> 결정적이지 않고 확률적으로 텍스트 생성

temperature 값이 낮을수록 더 결정적인 샘플링

## RNN 확장

### 적층 순환 네트워크

- 지금까지 하나의 LSTM 층이 포함된 네트워크를 보았지만 LSTM 층을 쌓은 네트워크도 훈련 가능
- 첫 번째 LSTM 층의 return_sequences 매개변수를 True로 하여 모든 타임스텝의 은닉 상태를 출력 후 두 번째 LSTM 층이 첫 번째 층의 은닉 상태를 입력 데이터로 사용

### GRU 층

**LSTM 셀과 주요 차이점**

1. 삭제 게이트와 입력 게이트가 리셋 게이트와 업데이트 게이트로 바뀜
2. 셀 상태와 출력 게이트가 없음 셀은 은닉 상태만 출력

### 양방향 셀

- 후진 방향으로도 처리 -> 두 개의 은닉 상태 사용
- 만들어진 층의 은닉 상태는 셀 유닛 개수의 두 배 길이를 가진 벡터

## 인코더-디코더 모델

이런 훈련 방식을 **티처 포싱**이라 한다

**아래 종류의 작업에 대한 예**

- 언어 번역 : 네트워크에 소스 언어로 된 텍스트를 주입하고 타깃 언어로 번역된 텍스트를 출력한느 것이 목표
- 질문 생성 : 네트워크에 텍스트 문장을 주입하고 텍스트에 관해 가능한 질문을 생성하는 것이 목적
- 텍스트 요약 : 네트워크에 긴 텍스트 문장을 주입하고 이 문장의 짧은 요약을 생성하는 것이 목적

**순차 데이터에서 인코더-디코더 과정**

1. 원본 입력 시퀀스는 인코더 RNN에 의해 하나의 벡터로 요약

2. 이 벡터는 디코더 RNN의 초깃값으로 사용

3. 각 타임스텝에서 디코더 RNN의 은닉 상태는 완전 연결 층에 연결되어 단어 어휘 사전에 대한 확률 분포를 출력 

   -> 이런 식으로 인코더가 생성한 입력 데이터로 디코더를 초기화한 다음 새로운 텍스트 시퀀스 생성 가능

- 마지막 은닉 상태는 전체 입력 문서에 대한 하나의 표현 

- 그 다음 디코더는 이 표현을 순차적인 출력으로 변환

## 질문-대답 생성기

**이 모델은 두 개의 부분으로 구성**

- RNN 하나가 텍스트로부터 대답 후보를 고름
- 인코더-디코더 네트워크가 RNN이 선택한 대답 후보 중 하나가 주어졌을 때 적절한 질문을 생성

### 예측

**이전에 본 적 없는 입력 문서 시퀀스에서 모델을 테스트하기 위해 다음 과정을 수행해야 함**

1. 대답 생성기에 문서 텍스트를 주입하여 대답의 샘플 위치 생성
2. 이 대답 중 하나를 선택하여 인코더-디코더 질문 생성기로 전달
3. 문서와 대답 마스크를 인코더에 주입하여 디코더의 초기 상태 생성
4. 이 초기 상태로 디코더 초기화 후 <START> 토큰을 주입하여 질문의 첫 번째 단어 생성
5. <END> 토큰을 예측할 때 까지 하나씩 생성된 단어를 주입하여 이 과정을 반복